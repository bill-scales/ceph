// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
// vim: ts=8 sw=2 smarttab
#pragma once

#include "IoOp.h"

#include <boost/asio/io_context.hpp>

#include "librados/librados_asio.h"

#include "include/interval_set.h"
#include "global/global_init.h"
#include "global/global_context.h"
#include "common/Thread.h"

/* Overview
 *
 * class IoExerciser
 *   Virtual class. IoExerciser's apply IoOps generated by an
 *   IoSequence, they can choose how many I/Os to execute in
 *   parallel and scale up the size of I/Os by the blocksize
 *
 * class ObjectModel
 *   An IoExerciser. Tracks the data stored in an object, applies
 *   IoOp's to update the model. Polices that I/Os that are
 *   permitted to run in parallel do not break rules. Provides
 *   interface to query state of object. State can be encoded
 *   and decoded
 *
 * class RadosIo
 *   An IoExerciser. A simple RADOS client that generates I/Os
 *   from IoOps. Uses an ObjectModel to track the data stored
 *   in the object. Uses DataBuffer to create and validate
 *   data buffers. When there are not barrier I/Os this may
 *   issue multiple async I/Os in parallel.
 */

namespace Ceph {
  namespace DataGenerator {
    class DataGenerator;
  }
}

class IoExerciser
{
protected:
  int num_io;
  const std::string oid;
  uint64_t block_size;

public:
  IoExerciser(const std::string oid, uint64_t block_size) :
    num_io(0), oid(oid), block_size(block_size) {}

  virtual ~IoExerciser() = default;

  /* Return true if the next I/O can be applied without blocking */
  virtual bool readyForIoOp(IoOp& op) = 0;

  /* Apply next I/O, may block waiting for previous I/Os to complete
   * unless readyForIoOp returned true
   */
  virtual void applyIoOp(IoOp& op) = 0;

  const uint64_t get_block_size() const
  {
    return block_size;
  }

  const std::string get_oid()
  {
    return oid;
  }

  int get_num_io()
  {
    return num_io;
  }
};

/* Model of an object to track its data contents */

class ObjectModel: public IoExerciser {
private:
  bool created;
  std::vector<int> contents;
  ceph::util::random_number_generator<int> rng =
    ceph::util::random_number_generator<int>();

  // Track read and write I/Os that can be submitted in
  // parallel to detect violations:
  //
  // * Read may not overlap with a parallel write
  // * Write may not overlap with a parallel read or write
  // * Create / remove may not be in parallel with read or write
  //
  // Fix broken test cases by adding barrier ops to restrict
  // I/O exercisers from issuing conflicting ops in parallel
  interval_set<uint64_t> reads;
  interval_set<uint64_t> writes;
public:
  ObjectModel(const std::string oid, uint64_t block_size, int seed) :
    IoExerciser(oid, block_size), created(false)
  {
    rng.seed(seed);
  }

  bool readyForIoOp(IoOp& op)
  {
    return true;
  }

  void applyIoOp(IoOp& op)
  {
    switch (op.op) {
    case IO_OP_BARRIER:
      reads.clear();
      writes.clear();
      break;

    case IO_OP_CREATE:
      ceph_assert(!created);
      ceph_assert(reads.empty());
      ceph_assert(writes.empty());
      created = true;
      contents.resize(op.length1);
      for (uint64_t i = 0; i < contents.size(); i++) {
	contents[i] = rng();
      }
      break;

    case IO_OP_REMOVE:
      ceph_assert(created);
      ceph_assert(reads.empty());
      ceph_assert(writes.empty());
      created = false;
      contents.resize(0);
      break;

    case IO_OP_READ3:
      ceph_assert(created);
      ceph_assert(op.offset3 + op.length3 <= contents.size());
      // Not allowed: read overlapping with parallel write
      ceph_assert(!writes.intersects(op.offset3, op.length3));
      reads.union_insert(op.offset3, op.length3);
      [[fallthrough]];
    case IO_OP_READ2:
      ceph_assert(created);
      ceph_assert(op.offset2 + op.length2 <= contents.size());
      // Not allowed: read overlapping with parallel write
      ceph_assert(!writes.intersects(op.offset2, op.length2));
      reads.union_insert(op.offset2, op.length2);
      [[fallthrough]];
    case IO_OP_READ:
      ceph_assert(created);
      ceph_assert(op.offset1 + op.length1 <= contents.size());
      // Not allowed: read overlapping with parallel write
      ceph_assert(!writes.intersects(op.offset1, op.length1));
      reads.union_insert(op.offset1, op.length1);
      num_io++;
      break;
      
    case IO_OP_WRITE3:
      ceph_assert(created);
      // Not allowed: write overlapping with parallel read or write
      ceph_assert(!reads.intersects(op.offset3, op.length3));
      ceph_assert(!writes.intersects(op.offset3, op.length3));
      writes.union_insert(op.offset3, op.length3);
      for (uint64_t i = op.offset3; i < op.offset3 + op.length3; i++) {
	ceph_assert(i < contents.size());
	contents[i] = rng();
      }
      [[fallthrough]];
    case IO_OP_WRITE2:
      ceph_assert(created);
      // Not allowed: write overlapping with parallel read or write
      ceph_assert(!reads.intersects(op.offset2, op.length2));
      ceph_assert(!writes.intersects(op.offset2, op.length2));
      writes.union_insert(op.offset2, op.length2);
      for (uint64_t i = op.offset2; i < op.offset2 + op.length2; i++) {
	ceph_assert(i < contents.size());
	contents[i] = rng();
      }
      [[fallthrough]];
    case IO_OP_WRITE:
      ceph_assert(created);
      // Not allowed: write overlapping with parallel read or write
      ceph_assert(!reads.intersects(op.offset1, op.length1));
      ceph_assert(!writes.intersects(op.offset1, op.length1));
      writes.union_insert(op.offset1, op.length1);
      for (uint64_t i = op.offset1; i < op.offset1 + op.length1; i++) {
	ceph_assert(i < contents.size());
	contents[i] = rng();
      }
      num_io++;
      break;
    case IO_OP_FAILWRITE:
      ceph_assert(created);
      // Not allowed: write overlapping with parallel read or write
      ceph_assert(!reads.intersects(op.offset1, op.length1));
      ceph_assert(!writes.intersects(op.offset1, op.length1));
      writes.union_insert(op.offset1, op.length1);
      // No model update as the write fails
      num_io++;
      break;
    case IO_OP_APPEND:
      {
	uint64_t oldsize = contents.size();
	op.offset1 = oldsize;
	ceph_assert(created);
	contents.resize(oldsize + op.length1);
	writes.union_insert(oldsize, op.length1);
	for (uint64_t i = oldsize; i < contents.size(); i++) {
	  contents[i] = rng();
	}
	num_io++;
      }
      break;
    default:
      break;
    }
  }

  const int get_seed(uint64_t offset) const
  {
    ceph_assert(offset < contents.size());
    return contents[offset];
  }

  std::string to_string(int mask = -1)
  {
    if (!created) {
      return "Object does not exist";
    }
    std::string result = "{";
    for (uint64_t i = 0; i < contents.size(); i++) {
      if (i != 0) {
	result += ",";
      }
      result += std::to_string(contents[i] & mask);
    }
    result += "}";
    return result;
  }
    
  void encode(ceph::buffer::list& bl) const {
    ENCODE_START(1, 1, bl);
    encode(created, bl);
    if (created) {
      encode(contents, bl);
    }
    ENCODE_FINISH(bl);
  }    

  void decode(ceph::buffer::list::const_iterator& bl) {
    DECODE_START(1, bl);
    DECODE_OLDEST(1);
    decode(created, bl);
    if (created) {
      decode(contents, bl);
    } else {
      contents.resize(0);
    }
    DECODE_FINISH(bl);
  }
};

/* Simple RADOS I/O generator */

class RadosIo: public IoExerciser {
protected:
  librados::Rados& rados;
  boost::asio::io_context& asio;
  std::unique_ptr<ObjectModel> om;
  std::unique_ptr<Ceph::DataGenerator::DataGenerator> db;
  const std::string pool;
  int threads;
  ceph::mutex& lock;
  ceph::condition_variable& cond;
  librados::IoCtx io;
  int outstanding_io;

  void start_io();
  void finish_io();
  void wait_for_io(int count);
  
public:
  RadosIo(librados::Rados& rados,
	  boost::asio::io_context& asio,
	  const std::string pool,
	  const std::string oid,
	  uint64_t block_size,
	  int seed,
	  int threads,
	  ceph::mutex& lock,
	  ceph::condition_variable& cond);

  ~RadosIo();

  void allow_ec_overwrites(bool allow);
  void allow_ec_optimizations(bool allow);

  class AsyncOp {
  public:
    RadosIo *r;
    librados::ObjectReadOperation rop;
    librados::ObjectWriteOperation wop;
    ceph::buffer::list bl1;
    ceph::buffer::list bl2;
    ceph::buffer::list bl3;
    uint64_t offset1;
    uint64_t length1;
    uint64_t offset2;
    uint64_t length2;
    uint64_t offset3;
    uint64_t length3;

    AsyncOp(RadosIo *r,
            uint64_t offset1 = 0, uint64_t length1 = 0,
	    uint64_t offset2 = 0, uint64_t length2 = 0,
	    uint64_t offset3 = 0, uint64_t length3 = 0 );

    ~AsyncOp();
  };

  // Must be called with lock held
  bool readyForIoOp(IoOp& op);
  
  void applyIoOp(IoOp& op);
};
